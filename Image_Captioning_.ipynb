{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Captioning .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnImkUeDN4oBCMaKNIERW5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanchess98/Notebooks-Deep-Learning/blob/Image-captioning/Image_Captioning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Captioning\n",
        "El image captioning es el proceso de generar una descripción textual a una imagen dada. Esto ha sido una tarea muy importante y fundamental en el campo del Deep Learning. El rango de aplicaciones del image captioning es muy amplio. En recomendacion en aplicaciones de edición, uso de asistentes virtuales, indexación de imagenes, para personas con discapacidad visual, en las redes sociales y en otras aplicaciones de procesamiento del lenguaje natural.\n",
        "\n",
        "Image Captioning puede ser considerado como un problema Sequence-to-Sequence ya que convierte imagenes, lo cual es considerado como una secuencia de pixeles a una secuencia de palabras. Para lograr este cometido, necesitamos procesar tanto el lenaguenje como las imagenes. Para el lenguaje, se usan redes neuronales recurrentes y para las imagenes, se usan redes neuronales convolucionales para obtener el vector de características respectivamente.\n"
      ],
      "metadata": {
        "id": "saH1JWluk_MI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si se nos dice que describamos la imagen, uno podría describirla como \"un perro sobre una toalla azul\" o como \" un perro marrón jugando con una pelota verde\". Así que, cómo estamos haciendo esto? Mientras generamos la descripción, estamos mirando la imagen para entender lo que pasa en ella pero al mismo tiempo estamos buscando por sequencias de palabras con sentido. De la primera parte se encarga una CNN y de la segunda una RNN.\n",
        "\n",
        "Si pudieramos obtener un dataset con imagenes y su correspondiente descripción realizada por un humano, podemos entrenar redes que realicen esta tarea automaticamente. Datasets como FLICKR 8K, FLICKR 30K y MS-COCO son algunos de los más usados para este tipo de aplicaciones.\n",
        "\n",
        "Ahora viene un punto importante aquí, anteriormente hemos visto que podemos describir una misma imagen de diferentes maneras, entonces cómo evaluamos que tan bien nuestro modelo está realizando esta tarea? Para problemas del tipo sequence-to-sequence como resumen, traducción o descripción se usa una métrica llamada el puntaje BLEU. BLEU se refiere a Bilingual Evaluation Understudy y es una métrica para evualuar una oración generada con respecto a una de referencia. La puntuación va de 0.0 a 1.0 para un completo dismatch a un match perfecto. Los invito a leer más sobre esta métrica, la cual es muy utlizada para evaluar los sistemas de traducción.\n",
        "\n",
        "Hemos visto que tenemos que crear una red neuronal multimodal que utilice los vectores de características generados por la CNN y la RNN, así que esto se traduce en que tendremos 2 entradas. Una es la imagen que queremos describir y la otra son las palabras en la sequencia de texto producida hasta ahora como una secuencia como entrada la RNN. \n",
        "\n",
        "Estamos lidiando con dos tipos de información una de tipo de lenguaje y otra de tipo imagen. Así que la la cuestion que surge es en cuál orden se deberia introducir cada una de estas partes de información en el modelo? Hablando de manera elaborada, se necesita un modelo RNN porque queremos generar una secuencia de palabras, asi que, cúando deberia introducirse los vectores de los datos de la imagen en el modelo que procesa el lenguaje?\n",
        "\n",
        "\n",
        "\n",
        "Source:\n",
        "https://towardsdatascience.com/a-guide-to-image-captioning-e9fd5517f350"
      ],
      "metadata": {
        "id": "uNTcnutApkgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x-rKAG4zo6qK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hMA1kge9q-C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import os \n",
        "import sys\n",
        "from pycocotools.coco import COCO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/opt/cocoapi/PythonAPI')\n",
        "\n",
        "# initialize COCO API for instance annotations\n",
        "dataDir = '/home/Project/Udacity-Computer-Vision-Nanodegree-Program/project_2_image_captioning_project/cocoapi'\n",
        "dataType = 'val2014'\n",
        "instances_annFile = os.path.join(dataDir, 'annotations/instances_{}.json'.format(dataType))\n",
        "coco = COCO(instances_annFile)\n",
        "\n",
        "# initialize COCO API for caption annotations\n",
        "captions_annFile = os.path.join(dataDir, 'annotations/captions_{}.json'.format(dataType))\n",
        "coco_caps = COCO(captions_annFile)\n",
        "\n",
        "# get image ids \n",
        "ids = list(coco.anns.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "5ZyJvk4rqzNf",
        "outputId": "7de348c9-98f7-4fcb-c348-84a69dab201a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-07a53198eb5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'val2014'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minstances_annFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annotations/instances_{}.json'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances_annFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# initialize COCO API for caption annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading annotations into memory...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annotation file format {} not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/Project/Udacity-Computer-Vision-Nanodegree-Program/project_2_image_captioning_project/cocoapi/annotations/instances_val2014.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GkY1rXMwk-P8"
      }
    }
  ]
}