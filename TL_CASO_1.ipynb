{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juanchess98/Notebooks-Deep-Learning/blob/transfer_learning/TL_CASO_1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning (Enfoque por cuadrantes o escenarios usando Pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprender una tarea nueva puede ser un proceso que lleve una considerable cantidad de tiempo. Sin embargo, los seres humanos tienen la capacidad de asociar nuevas tareas con otras que ya sabe como ejecutar y aplicar este conocimiento para reducir el tiempo y costo de aprendizaje. Por ejemplo, si una persona sabe como manejar una bicicleta, para esa persona será mucho más fácil aprender a manejar una motocicleta. Asimismo, una persona que domine el idioma español tendrá mucha más facilidad para aprender otro idioma similar, como el portugués, que una persona de origen alemán o japonés.\n",
    "\n",
    "Este mismo concepto se puede aplicar a las máquinas inteligentes y se le conoce como *Transfer Learning* o Aprendizaje por transferencia. En este caso, un modelo desarrollado para una resolver una tarea puede usarse como punto de partida para resolver otra tarea similar.\n",
    "\n",
    "Pueden presentarse diferentes escenarios en la aplicación de *transferencia de aprendizaje* a un dataset nuevo, pero generalmente se puede reducir a cuatro casos:\n",
    "\n",
    "1. La nueva base de datos es numerosa y se parece muy poco a la base de datos del modelo pre-entrenado\n",
    "2. La nueva base de datos es numerosa y parecida a la base de datos del modelo pre-entrenado\n",
    "3. La nueva base de datos es pequeña y diferente de la base de datos del modelo pre-entrenado\n",
    "4. La nueva base de datos es pequeña y parecida a la base de datos del modelo pre-entrenado\n",
    "\n",
    "En la siguiente gráfica tipo plano cartesiano pueden visualizarce mejor los posibles escenarios, dividiendo el plano en cuadrantes que representan cada escenario y dónde en la dirección de crecimiento del eje $x$ se indica la similaridad de los dos datasets y el eje $y$ el tamaño del nuevo dataset.\n",
    "\n",
    "**(Insertar imagen de cuadrantes aquí)**\n",
    "\n",
    "Para cada escenario existe una metodología diferente a implementar y en el curso de este notebook trataremos de ejemplificar cada una con la ayuda de los frameworks de Keras y Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso 1. La nueva base de datos es numerosa y diferente a la del modelo pre-entrenado\n",
    "\n",
    "Si la nueva base de datos es grande y no se parece a la original, se recomienda seguir los siguientes pasos:\n",
    "\n",
    "1. Quitar la última capa totalmente conectada y añadir una nueva capa totalmente conectada cuya dimensión de salida se igual al número de clases de la nueva base de datos.\n",
    "\n",
    "2. Aleatorizar los pesos de la nueva capa totalmente conectada e inicializarlos con valores aleatorios\n",
    "\n",
    "3. Entrenar la red para actualizar los pesos de la nueva capa totalmente conectada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar la base de datos DIFERENTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloaders = trainloader\n",
    "                '''for inputs, labels in trainloader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)'''\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                dataloaders = testloader\n",
    "                '''for inputs, labels in testloader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)'''\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    outputs = model(inputs).to(device)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        #loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            '''epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())'''\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el modelo preentrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = models.vgg16(pretrained=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
